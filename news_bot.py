import feedparser
import google.generativeai as genai
import pymysql
import requests
import json
import os
import sys
import html
import time
from datetime import datetime
from bs4 import BeautifulSoup
import newspaper

# ==========================================
# [ì‚¬ìš©ì ì„¤ì •]
# ==========================================

# 1. API í‚¤ ì„¤ì •
GEMINI_API_KEY = ""
TELEGRAM_TOKEN = ""
CHAT_ID = ""

# 2. MySQL ë°ì´í„°ë² ì´ìŠ¤ ì ‘ì† ì •ë³´
DB_CONFIG = {
    'host': 'localhost',
    'port': ,
    'user': '',
    'password': '',
    'db': '',
    'charset': 'utf8',
}

# 3. ê¸°íƒ€ ì„¤ì •
HISTORY_FILE = "seen_posts.json"
MAX_HISTORY = 1000

# ==========================================
# [ì‹œìŠ¤í…œ ë¡œì§]
# ==========================================

genai.configure(api_key=GEMINI_API_KEY)

try:
    model = genai.GenerativeModel('gemini-2.5-flash') 
except Exception as e:
    print(f"âš ï¸ ëª¨ë¸ ì´ˆê¸°í™” ê²½ê³ : {e}")

# --- 1. DBì—ì„œ RSS ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ---
def get_rss_list_from_db():
    conn = None
    try:
        conn = pymysql.connect(**DB_CONFIG)
        with conn.cursor() as cursor:
            cursor.execute("SELECT url, created_at FROM rss_feeds")
            result = cursor.fetchall()
            return result
    except Exception as e:
        print(f"âŒ DB ì ‘ì†/ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return []
    finally:
        if conn:
            conn.close()

# --- 2. ê¸°ë¡ ê´€ë¦¬ ---
def load_history():
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            return []
    return []

def save_history(history_list):
    if len(history_list) > MAX_HISTORY:
        history_list = history_list[-MAX_HISTORY:]
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(history_list, f, ensure_ascii=False, indent=4)

# --- 3. ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ (import newspaper ë°©ì‹ ì ìš©) ---
def get_article_content(url, entry=None):
    text = ""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://www.google.com/'
        }
        
        response = requests.get(url, headers=headers, timeout=15)
        
        if response.status_code != 200:
            print(f"      âš ï¸ [ì ‘ì† ì‹¤íŒ¨] Status Code: {response.status_code} -> RSS ìš”ì•½ë³¸ ì‹œë„")
        else:
            # 1ì°¨: Newspaper4k
            try:
                # [ìˆ˜ì •] ê³µì‹ ë¬¸ì„œì— ë”°ë¼ input_htmlì„ ì¸ìë¡œ ì „ë‹¬í•˜ì—¬ ë‹¤ìš´ë¡œë“œ ìƒëµ ë° ìë™ íŒŒì‹±
                article = newspaper.article(url, language='ko', input_html=response.text)
                text = article.text.strip()
                print(f"      ğŸ‘‰ [1ì°¨ íŒŒì‹±(Newspaper4k)] ë³¸ë¬¸ ê¸¸ì´: {len(text)}ì")
            except Exception as e:
                print(f"      âš ï¸ Newspaper4k íŒŒì‹± ì—ëŸ¬: {e}")
            
            # 2ì°¨: BS4
            if len(text) < 100:
                print("      ğŸ”§ [2ì°¨ íŒŒì‹±(BS4)] Newspaper4k ì‹¤íŒ¨ -> BeautifulSoup ì‹œë„")
                soup = BeautifulSoup(response.text, 'html.parser')
                
                for tag in soup(["script", "style", "nav", "footer", "header", "iframe", "noscript"]):
                    tag.decompose()
                
                main_content = soup.find('article') or soup.find('main') or soup
                paragraphs = main_content.find_all('p')
                bs_text = ' '.join([p.get_text().strip() for p in paragraphs])
                
                if len(bs_text) < 100:
                    bs_text = soup.get_text(separator=' ', strip=True)
                
                text = bs_text
                print(f"      ğŸ‘‰ [2ì°¨ íŒŒì‹± ê²°ê³¼] ë³¸ë¬¸ ê¸¸ì´: {len(text)}ì")

    except Exception as e:
        print(f"      âš ï¸ í¬ë¡¤ë§ ì—ëŸ¬: {e}")

    # 3ì°¨: RSS ìš”ì•½ë³¸
    if (not text or len(text) < 50) and entry:
        print("      ğŸ”„ [ëŒ€ì²´] íŒŒì‹± ì‹¤íŒ¨/ì°¨ë‹¨ë¨ -> RSS ìš”ì•½ë³¸ ì‚¬ìš©")
        if hasattr(entry, 'summary'): text = entry.summary
        elif hasattr(entry, 'description'): text = entry.description
        if len(text) < 10: text = None

    return text

# --- 4. AI ìš”ì•½ ---
def summarize_article(text, original_title):
    if not text:
        print("      âš ï¸ [ìš”ì•½] ë³¸ë¬¸ ì—†ìŒ (Skip)")
        return None
    
    print("      âœ¨ [ìš”ì•½] AI ë¶„ì„ ë° ë²ˆì—­ ì¤‘...")
    
    prompt = f"""
    Analyze the following news article and provide the output in strict JSON format.
    
    [Original Title]: {original_title}
    [Text]: {text[:3500]}
    
    Output JSON with these keys:
    1. "original_summary": A 3-sentence summary in the **original language**.
       - **Important**: Summarize the content directly as objective facts or narrative. 
       - **Avoid** phrases like "The author argues...", "The article discusses...", or "According to the text...".
       
    2. "korean_title": The title translated into **Korean**.
    
    3. "korean_summary": The summary translated into **Korean**.
       - **Important**: Use the same direct style as above (no "í•„ìëŠ”...", "ê¸°ì‚¬ëŠ”...").
       - End sentences with a noun form or completed style (e.g., "~í•¨", "~í–ˆìŒ" or polite "~ë‹ˆë‹¤").
    
    Do not use markdown code blocks. Just output the raw JSON string.
    """
    try:
        response = model.generate_content(prompt)
        response_text = response.text
        
        # JSON ì¶”ì¶œ ë¡œì§
        start_idx = response_text.find('{')
        end_idx = response_text.rfind('}')
        
        if start_idx != -1 and end_idx != -1 and start_idx < end_idx:
            json_str = response_text[start_idx : end_idx + 1]
            return json.loads(json_str)
        else:
            cleaned_text = response_text.replace("```json", "").replace("```", "").strip()
            return json.loads(cleaned_text)
            
    except Exception as e:
        print(f"      âŒ [AI ì—ëŸ¬] : {e}")
        return None

# --- 5. í…”ë ˆê·¸ë¨ ì „ì†¡ ---
def send_telegram(title, summary_data, link, source_name, pub_date):
    title = html.unescape(title)
    source_name = html.unescape(source_name)

    if summary_data and isinstance(summary_data, dict):
        orig_summary = html.unescape(summary_data.get('original_summary', 'ìš”ì•½ ì—†ìŒ'))
        kr_title = html.unescape(summary_data.get('korean_title', 'ì œëª© ë²ˆì—­ ë¶ˆê°€'))
        kr_summary = html.unescape(summary_data.get('korean_summary', 'ë‚´ìš© ë²ˆì—­ ë¶ˆê°€'))

        message = (
            f"ğŸ“¢ {title}\n"
            f"âœ… summary:{orig_summary}\n\n"
            f"ğŸ“¢ {kr_title}\n"
            f"âœ… ì£¼ìš”ë‚´ìš©:{kr_summary}\n\n"
            f"â­•ï¸ {source_name}\n"
            f"ğŸ“… ë°œí–‰ì¼ : {pub_date}\n"
            f"ğŸ”— {link}"
        )
    else:
        message = (
            f"ğŸ“¢ {title}\n"
            f"â­•ï¸ {source_name}\n"
            f"ğŸ“… ë°œí–‰ì¼ : {pub_date}\n"
            f"ğŸ”— {link}"
        )
    
    try:
        requests.post(f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage", json={"chat_id": CHAT_ID, "text": message})
    except Exception as e:
        print(f"âŒ ì „ì†¡ ì‹¤íŒ¨: {e}")

# --- ë©”ì¸ ì‹¤í–‰ ---
def main():
    print(f"\n======== ë‰´ìŠ¤ í´ë¦¬í•‘ ì‹œì‘ ({datetime.now()}) ========")
    
    rss_data_list = get_rss_list_from_db()
    
    if not rss_data_list:
        print("âš ï¸ DBì— ë“±ë¡ëœ RSSê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"âœ… êµ¬ë… ì¤‘ì¸ ì±„ë„: {len(rss_data_list)}ê°œ")

    seen_links = load_history()
    new_links_count = 0

    for rss_url, feed_created_at in rss_data_list:
        print(f"\nğŸ“¡ ê²€ìƒ‰ ì¤‘: {rss_url}")
        try:
            feed = feedparser.parse(rss_url)
            source_title = feed.feed.title if 'title' in feed.feed else "News"
            
            # ëª¨ë“  ê¸€ì„ ìˆœíšŒ (ìµœì‹ ìˆœ í•„í„°ë§ì€ ë‚´ë¶€ ë¡œì§ì—ì„œ ì²˜ë¦¬)
            for entry in feed.entries:
                link = entry.link
                
                if link in seen_links:
                    continue

                # ê¸€ ì‘ì„± ì‹œê°„ íŒŒì•…
                entry_date = None
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    entry_date = datetime.fromtimestamp(time.mktime(entry.published_parsed))
                elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                    entry_date = datetime.fromtimestamp(time.mktime(entry.updated_parsed))
                
                # RSS DB ë“±ë¡ì¼ í•„í„°ë§ (ë“±ë¡ì¼ ì´ì „ ê¸€ ë¬´ì‹œ)
                if entry_date and feed_created_at:
                    if entry_date < feed_created_at:
                        continue

                print(f"   ğŸ†• ìƒˆ ê¸€ ë°œê²¬! : {entry.title}")
                
                pub_date_str = entry_date.strftime('%Yë…„ %mì›” %dì¼') if entry_date else "ë‚ ì§œ ì •ë³´ ì—†ìŒ"

                content = get_article_content(link, entry)
                
                summary_data = None
                if content:
                    summary_data = summarize_article(content, entry.title)
                
                send_telegram(entry.title, summary_data, link, source_title, pub_date_str)
                
                seen_links.append(link)
                new_links_count += 1
                
        except Exception as e:
            print(f"   âŒ RSS íŒŒì‹± ì˜¤ë¥˜: {e}")
    
    if new_links_count > 0:
        save_history(seen_links)
        print(f"\nâœ… ì´ {new_links_count}ê°œì˜ ë‰´ìŠ¤ë¥¼ ì „ì†¡í–ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nğŸ’¤ ìƒˆë¡œ ì—…ë°ì´íŠ¸ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
